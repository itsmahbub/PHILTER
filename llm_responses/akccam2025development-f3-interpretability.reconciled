{"value":"Low","why":"While the model includes an internal Attention layer, the paper never uses or presents the resulting attention weights to explain individual decisions. There are no per-decision heatmaps, feature-importance plots, or any post-hoc XAI methods described. The architecture is treated purely as a performance-improving mechanism, with no mechanism exposed to audit or interpret each prediction’s rationale.","evidence":"Page 14, Section 4.3.1–4.3.2 (“Proposed Architecture” and “Attention Layer”): attention is described only as a way to “focus on specific parts” of the BiLSTM output, but no use of those weights for explanation is shown. \nPage 1, Abstract: no mention of interpretability or explanation methods anywhere in the paper."}