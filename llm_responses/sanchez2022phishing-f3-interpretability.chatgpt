{"value":"Low","why":"The paper does not provide any per-decision or global explanation mechanisms. Neither inherently interpretable decision paths nor post-hoc XAI methods (e.g. SHAP, LIME, attention or gradient heatmaps) are applied or discussed. Models (including logistic regression, tree ensembles, and CNNs) are evaluated solely by performance metrics without feature‐importance analysis or explanation of individual predictions.","evidence":"Throughout Sections IV–VI (e.g., pp. 5–7), the authors describe feature extraction (handcrafted and TF–IDF N-grams) and model training (LR, XGBoost, CNN, etc.) and only report accuracy/F1/precision/recall. There is no mention of extracting or presenting feature importances, decision paths, or any XAI technique to explain individual classifications."}