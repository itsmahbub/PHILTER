```json
{
    "value": "High",
    "why": "The paper demonstrates a strong resistance to active attacks by explicitly evaluating its models against a diverse set of adversarial techniques relevant to its multimodal nature. The authors first identify the framework's vulnerability to attacks such as visual obfuscations (image distortions), URL cloaking (dynamic redirects), HTML obfuscation (script injections), and standard adversarial perturbations generated by methods like Projected Gradient Descent (PGD) and Fast Gradient Sign Method (FGSM). They then incorporate specific defense mechanisms, primarily perturbation-resistant adversarial training and adaptive graph augmentation, to mitigate these threats. The paper quantifies the success of these defenses, reporting a 5.6% improvement in accuracy against adversarial attacks, thereby demonstrating a robust and intentional design for resilience.",
    "evidence": "Page 12, Section IV, \"Upon testing, it was found that the application of adversarial perturbation methods (like Projected Gradient Descent (PGD) and Fast Gradient Sign Method (FGSM)) later caused a drop in phishing detection by 7.8%, showing susceptibility to adversarially targeted phishing pages. To secure the architectures, perturbation-resistant embedding-based adversarial training is incorporated into MM-HFN and TGNN-Att. In essence, contrastive adversarial training generates artificially perturbed phishing samples that force the model to learn resilient feature representations. This method improves phishing detection accuracy against adversarial attacks by 5.6% to inhibit evasion methods.\""
}
```