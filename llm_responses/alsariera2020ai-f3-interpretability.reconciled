{"value":"Low","why":"Although the authors assert that their Extra-Trees–based meta-learners are ‘interpretable’, the paper contains no concrete mechanism for inspecting or explaining individual or global model decisions. There is no per-decision traceability (no decision paths, SHAP/LIME, or attention maps) and no disclosure of global insights such as feature‐importance scores or information‐gain rankings. Under the codebook, this absence of any explanatory or auditing technique places the work in the ‘Low’ interpretability category.","evidence":"Page 8, Section IV.B: Evaluation metrics are limited to Accuracy, False Positive, False Negative, and F-measure—no explanatory metrics are reported. Page 10, Section V: Though the authors claim “this study presented AI phishing detection methods that are interpretable,” they do not provide any method (e.g., feature importance, decision path exposure, or post hoc XAI) to support this claim."}