```json
{
    "value": "Medium",
    "why": "The paper explicitly designs and evaluates a defense against a significant class of active attacks: adversarial perturbations against its deep learning models. The proposed defense, a 'step-ReLU' function, is designed to mask gradients and is evaluated against multiple state-of-the-art gradient-based attacks (DAG, DeepFool, i-FGSM, JSMA), showing high effectiveness in preserving model accuracy. The authors further test this defense against a more advanced gradient-recovery attack (BPDA), demonstrating continued, albeit partial, robustness. This shows a clear consideration for active attacks targeting the visual classification pipeline.\n\nHowever, the evaluation is confined to this specific subset of attacks. The paper does not address or evaluate other active attack vectors relevant to its modality, which involves rendering a webpage in a sandbox. For instance, it does not evaluate robustness against dynamic evasions where malicious content (like a login form) is deferred and only appears after user interaction, which would be missed by a static screenshot. While cloaking is mentioned during data collection, Phishpedia's defense against it is not evaluated. Therefore, the system demonstrates strong but partial robustness against active attacks.",
    "evidence": "Page 6, Section 3.3, Defending against adversarial attacks: \"Instead, we design a new simple adversarial defense technique to transform our Faster-RCNN and Siamese model to counter some of the well-known gradient-based adversarial attacks, while (i) still preserving the model performance, and (ii) not requiring additional (adversarial) training that increases the system complexity. Specifically, we replace the ReLU function in some layers of both models with a step ReLU function.\"\n\nPage 12, Section 5.6.1, Experiment on Gradient-based Technique: \"In this set of experiments, we apply state-of-the-art adversarial attacks on both the object detection model and the Siamese model... We use DAG adversarial attack [78] to evaluate the robustness of our object detection model... We select four adversarial attacks to evaluate the robustness of our Siamese model: DeepFool [48], i-FGSM [25], i-StepLL [34], and JSMA [24]... We observe that (i) our defense technique effectively defends against existing state-of-the-art adversarial attacks; and (ii) the accuracy of Phishpedia is well preserved and not affected by the defense technique.\"\n\nPage 12, Section 5.6.2, Experiment with Gradient-recovering Technique: \"In this experiment, we adopt a state-of-the-art gradient-recovering technique, BPDA (Backward Pass Differentiable Approximation) [12], to attack Phishpedia... The results are presented in Table 7, where we compare the model accuracy before and after the attacks. BPDA is seen to be effective for a small number of masked layers, but less so for a large number of masked layers.\""
}
```