{"value":"Low","why":"The proposed tiny-BERT Stacking model is trained and evaluated on a single static dataset using a random 80/20 split, with no temporal separation of older versus newer samples, and no mechanisms for continual learning, periodic retraining, or time-aware feature engineering to address evolving phishing tactics. Thus, it assumes phishing strategies remain static and does not evaluate resilience to concept drift.","evidence":"Page 2240, Section IV.A.1: “in the experiment, we divided the data set into training set and test set at a ratio of 8:2.”"}