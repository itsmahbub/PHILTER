# PHILTER

**PH**ishing detection literature **I**nspection via **L**LMs and **T**argeted **E**xpert **R**eview.

PHILTER is a transparent, scalable framework for assessing AI-based phishing website detection papers against seven deployment-relevant requirements: four **Functionality** metrics (F1â€“F4) and three **Security** metrics (S1â€“S3). LLMs extract evidence and draft rationales; **experts validate and finalize all labels**. Applying PHILTER to 55 studies reveals systemic gaps and trade-offs.

---

## Framework Overview

![Framework Overview](framework-overview.png)

## ðŸ“¦ Repository layout

PHILTER/
â”œâ”€ codebook/
â”‚  â”œâ”€ f1-coverage.txt
â”‚  â”œâ”€ f2-benign-diversity.txt
â”‚  â”œâ”€ f3-interpretability.txt
â”‚  â”œâ”€ f4-evaluation-thoroughness.txt
â”‚  â”œâ”€ s1-concept-drift.txt
â”‚  â”œâ”€ s2-active-attack.txt
â”‚  â””â”€ s3-privacy.txt
â”œâ”€ llm_responses/
â”œâ”€ papers/
â”œâ”€ scripts/
â”‚  â”œâ”€ assessments_table_expert.py
â”‚  â”œâ”€ fulfillment_by_accuracy.py
â”‚  â”œâ”€ fulfillment_by_category.py
â”‚  â”œâ”€ fulfillment_by_citation.py
â”‚  â”œâ”€ fulfillment_by_deployment_mode.py
â”‚  â”œâ”€ fulfillment_by_detection_mode.py
â”‚  â”œâ”€ fulfillment_by_input.py
â”‚  â”œâ”€ fulfillment_by_publication_year.py
â”‚  â”œâ”€ llm_vs_expert_agreement_rates.py
â”‚  â””â”€ llm_vs_expert_assessments.py
â”œâ”€ llm_assessment_pipeline.py
â”œâ”€ assessments.json # Contains LLM assessments and expert assessments
â”œâ”€ README.md
â””â”€ requirements.txt


## Quick start

Put the research papers on phishing website detection inside `papers` directory.

```bash
# 1) Create env (Python 3.10â€“3.12 recommended)
python -m venv .venv && source .venv/bin/activate

# 2) Install dependencies
pip install -r requirements.txt

# 3) Set OpenAI and Gemini API keys
export OPENAI_API_KEY=...
export GOOGLE_API_KEY=...

# 4) Run LLM-assisted prelimiary evaluation stage
python llm_assessment_pipeline.py -p . -m codebook/f1-coverage.txt -o assessments.json
```
