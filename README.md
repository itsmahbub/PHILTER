# PHILTER

**PH**ishing detection literature **I**nspection via **L**LMs and **T**argeted **E**xpert **R**eview.

PHILTER is a transparent, scalable framework for assessing AI-based phishing website detection papers against seven deployment-relevant requirements: four **Functionality** metrics (F1â€“F4) and three **Security** metrics (S1â€“S3). LLMs extract evidence and draft rationales; **experts validate and finalize all labels**. Applying PHILTER to 55 studies reveals systemic gaps and trade-offs.

---

## Framework Overview

![Framework Overview](framework-overview.png)

## ðŸ“¦ Repository layout

```bash
PHILTER/
â”œâ”€ codebook/
â”‚  â”œâ”€ f1-coverage.txt
â”‚  â”œâ”€ f2-benign-diversity.txt
â”‚  â”œâ”€ f3-interpretability.txt
â”‚  â”œâ”€ f4-evaluation-thoroughness.txt
â”‚  â”œâ”€ s1-concept-drift.txt
â”‚  â”œâ”€ s2-active-attack.txt
â”‚  â””â”€ s3-privacy.txt
â”œâ”€ llm_responses/
â”œâ”€ papers/
â”œâ”€ scripts/
â”‚  â”œâ”€ assessments_table_expert.py
â”‚  â”œâ”€ fulfillment_by_accuracy.py
â”‚  â”œâ”€ fulfillment_by_category.py
â”‚  â”œâ”€ fulfillment_by_citation.py
â”‚  â”œâ”€ fulfillment_by_deployment_mode.py
â”‚  â”œâ”€ fulfillment_by_detection_mode.py
â”‚  â”œâ”€ fulfillment_by_input.py
â”‚  â”œâ”€ fulfillment_by_publication_year.py
â”‚  â”œâ”€ llm_vs_expert_agreement_rates.py
â”‚  â””â”€ llm_vs_expert_assessments.py
â”œâ”€ llm_assessment_pipeline.py
â”œâ”€ assessments.json # Contains LLM assessments and expert assessments
â”œâ”€ README.md
â””â”€ requirements.txt
```

## Quick start

Put the research papers on phishing website detection inside `papers` directory.

```bash
# 1) Create env (Python 3.10â€“3.12 recommended)
python -m venv .venv && source .venv/bin/activate

# 2) Install dependencies
pip install -r requirements.txt

# 3) Set OpenAI and Gemini API keys
export OPENAI_API_KEY=...
export GOOGLE_API_KEY=...

# 4) Run LLM-assisted prelimiary evaluation stage
python llm_assessment_pipeline.py -p . -m codebook/f1-coverage.txt -o assessments.json
```

## Contents of assessments.json

This file contains the structured evaluation results for each phishing website detection paper analyzed in our study. Each entry corresponds to a single paper and includes:

- **Metadata**: Year, venue, citation count, method name, and paper key.  
- **Category & Inputs**: Detection strategy category (e.g., feature-based, similarity-based, identity-based, hybrid) and input requirements (e.g., URL, webpage content).  
- **Performance Metrics**: Reported accuracy, precision, recall, and F1, detection mode (real-time vs. offline), deployment mode (client-side vs. server-side).  
- **Codebook Metrics**: Evaluations under the PHILTER framework. For each metric, multiple assessments are stored:  
  - `manual`: Expertâ€™s final label.  
  - `manual_reasoning`: Explanation for the manual label.  
  - `chatgpt`: Assessment generated by ChatGPT (includes value, reasoning, and supporting evidence).  
  - `gemini`: Assessment generated by Gemini (includes value, reasoning, and supporting evidence). 
  - `arbitrator`: Final reconciled label after resolving disagreements (includes value, reasoning, and supporting evidence).

  Metrics include:  
  - **F1 â€“ Diversity of Phishing Tactics**  
  - **F2 â€“ Diversity of Benign Pages**  
  - **F3 â€“ Interpretability**  
  - **F4 â€“ Evaluation Transparency**  
  - **S1 â€“ Adaptation to Concept Drift**  
  - **S2 â€“ Resistance to Active Attacks**  
  - **S3 â€“ Privacy Preservation**
